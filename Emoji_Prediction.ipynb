{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emoji_Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekincanozcelik/YoutubeDataAnalysis/blob/master/Emoji_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z4NJFtj0-A-",
        "colab_type": "text"
      },
      "source": [
        "# **Emoji Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdT1iIujuD6s",
        "colab_type": "code",
        "outputId": "6ad2dd11-e1ba-44dd-cee4-d558fb6ba452",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UieJps0Z47F3",
        "colab_type": "code",
        "outputId": "ccba2fbd-f082-47d4-9c87-c7caf5c26d7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP0Wa2FFKGxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import nltk\n",
        "import re\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KCNN2h8oh4v",
        "colab_type": "code",
        "outputId": "0bce1347-7bcb-4f12-eac1-d74fc9f9e669",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "source": [
        "pip install easybert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting easybert\n",
            "  Downloading https://files.pythonhosted.org/packages/b1/0c/783b52c939a0b9faa3d4fca152343fb899bbefa9afa388d14d389db019db/easybert-1.0.3.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from easybert) (1.17.5)\n",
            "Collecting tensorflow-hub==0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/5c/6f3698513cf1cd730a5ea66aec665d213adf9de59b34f362f270e0bd126f/tensorflow_hub-0.4.0-py2.py3-none-any.whl (75kB)\n",
            "\r\u001b[K     |████▍                           | 10kB 17.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 20kB 22.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 30kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 40kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 51kB 11.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 61kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 71kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 5.0MB/s \n",
            "\u001b[?25hCollecting bert-tensorflow==1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\r\u001b[K     |████▉                           | 10kB 24.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 29.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 31.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 32.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 32.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 32.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from easybert) (7.0)\n",
            "Collecting tensorflow-gpu==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
            "\u001b[K     |████████████████████████████████| 345.2MB 50kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub==0.4.0->easybert) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub==0.4.0->easybert) (3.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->easybert) (1.27.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->easybert) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->easybert) (0.34.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->easybert) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->easybert) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->easybert) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->easybert) (0.9.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1->easybert) (1.0.8)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 43.2MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 56.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub==0.4.0->easybert) (45.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1->easybert) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1->easybert) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1->easybert) (1.0.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/30/6a/9bde648117ec7087c89a45de0a8b25aba21d54d3defd08cb24eacded875f/mock-4.0.1-py3-none-any.whl\n",
            "Building wheels for collected packages: easybert\n",
            "  Building wheel for easybert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for easybert: filename=easybert-1.0.3-cp36-none-any.whl size=7621 sha256=32df14bfd382130641a9c8e623d6ba9c53de2759afea4631c856da7b85bb46b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/00/9f/587ad878f58cb7255674e9027c546893e8a921b13cec954510\n",
            "Successfully built easybert\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.13.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-hub, bert-tensorflow, tensorboard, mock, tensorflow-estimator, tensorflow-gpu, easybert\n",
            "  Found existing installation: tensorflow-hub 0.7.0\n",
            "    Uninstalling tensorflow-hub-0.7.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.7.0\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed bert-tensorflow-1.0.1 easybert-1.0.3 mock-4.0.1 tensorboard-1.13.1 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1 tensorflow-hub-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snEwDWAKo-v3",
        "colab_type": "code",
        "outputId": "b84626aa-b8d2-4d75-db84-0b9242a986d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "from easybert import Bert\n",
        "bert = Bert(\"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qanwUr861Kcl",
        "colab_type": "text"
      },
      "source": [
        "**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxy_IY1pSzZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing datasets#\n",
        "#Train set\n",
        "file1 = open(\"/content/drive/My Drive/Emoji_prediction/us_train.text\", \"r+\", encoding=\"utf-8\")\n",
        "train_text = file1.readlines()\n",
        "file2 = open(\"/content/drive/My Drive/Emoji_prediction/us_train.labels\", \"r+\", encoding=\"utf-8\")\n",
        "train_labels = file2.readlines()\n",
        "#Development set\n",
        "file3 = open(\"/content/drive/My Drive/Emoji_prediction/us_dev.text\", \"r+\", encoding=\"utf-8\")\n",
        "dev_text = file3.readlines()\n",
        "file4 = open(\"/content/drive/My Drive/Emoji_prediction/us_dev.labels\", \"r+\", encoding=\"utf-8\")\n",
        "dev_labels = file4.readlines()\n",
        "#Test set\n",
        "file5 = open(\"/content/drive/My Drive/Emoji_prediction/us_test.text\", \"r+\", encoding=\"utf-8\")\n",
        "test_text = file5.readlines()\n",
        "file6 = open(\"/content/drive/My Drive/Emoji_prediction/us_test.labels\", \"r+\", encoding=\"utf-8\")\n",
        "test_labels = file6.readlines()\n",
        "\n",
        "train_set = []\n",
        "dev_set = []\n",
        "test_set = []\n",
        "    \n",
        "for i in range(len(train_text)):\n",
        "    train_set.append((train_text[i].replace(\"\\n\", \"\"), train_labels[i].replace(\"\\n\", \"\")))\n",
        "for i in range(len(dev_text)):\n",
        "    dev_set.append((dev_text[i].replace(\"\\n\", \"\"), dev_labels[i].replace(\"\\n\", \"\")))\n",
        "for i in range(len(test_text)):\n",
        "    test_set.append((test_text[i].replace(\"\\n\", \"\"), test_labels[i].replace(\"\\n\", \"\")))\n",
        "\n",
        "#Shuffle datasets to avoid overfitting\n",
        "random.shuffle(train_set)\n",
        "random.shuffle(test_set)\n",
        "random.shuffle(dev_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn7h82Ovhg8I",
        "colab_type": "code",
        "outputId": "99538fcc-6a02-4de0-9c00-d04c5c747d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "print(train_set[:5])\n",
        "print(dev_set[:5])\n",
        "print(test_set[:5])\n",
        "print(len(train_text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Young @user killed it tonight . Proud of you always but boy you impressed me tonight . … ', '9'), ('ZEDS DEAD @user #hardsummermusicfestival #hardsummer2016… ', '1'), (\"#Valor launch party #BeautyandPinups #PinupsforVets ️ (@ Pearl's Liquor Bar - @user in West Hollywood, CA) \", '0'), ('Pajama night with the fam! ️ you all. #Christmas #nanashouse #pjs @ Jonesboro, Arkansas ', '0'), ('Workout done!! Thank you lavidafit !!! #fitness #workout… ', '9')]\n",
            "[('happy Friyay from us @ Indianola High School', '19'), ('Spending mothers day with my number 1 person @ Busch Stadium', '0'), ('@user is privileged to offer our biggest congratulations to each and every deserving…', '0'), (\"January! @ Mac's Wood Grilled\", '0'), ('Beautiful Miami .. @ Smith &amp; Wollensky Miami Steakhouse', '12')]\n",
            "[(\"Family for Life @ Macky's Bayside Bar &amp; Grill\", '0'), ('Shots from our pre-Turkey day show @user Patrick Gregory @ Urban Lounge', '10'), ('I forgot my iPhone on stage before @user I so deserved to find these on my pictures…', '2'), ('What a Time to be Alive @ Venice Beach', '15'), ('Checking out the #punkrockfleamarket today. @ Trenton Punk Rock…', '6')]\n",
            "157311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjfUuVgNrmAL",
        "colab_type": "code",
        "outputId": "c7bd0a36-3cd7-4008-8f2a-71ae65391312",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "####TEXT PREPROCESSING####\n",
        "#Initalizing lemmatizer and creating a list of stopwords\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "\n",
        "#Cleans reviews, removes stopwords and lemmatize to put the reviews in a form that machine can perform better\n",
        "def clean_reviews(string):\n",
        "    lemmatized_string = \"\"\n",
        "    #Remove HTML information from the string\n",
        "    processed_string = re.compile(r\"<[^>]+>\").sub(\" \", string)\n",
        "    \n",
        "    #Remove URLS from the string\n",
        "    processed_string = re.compile(r\"https?://[A-Za-z0-9./]+\").sub(\" \", processed_string)\n",
        "    \n",
        "    #Remove digits and punctuations\n",
        "    processed_string = re.compile(r\"[^a-zA-Z ]\").sub(\" \", processed_string)\n",
        "    \n",
        "    #Lowercase all the words\n",
        "    processed_string = processed_string.lower()\n",
        "    \n",
        "    #Does not append stopwords but appends other strings while lemmatizing them\n",
        "    for word in processed_string.split():\n",
        "        if word in stopwords:\n",
        "            continue\n",
        "        else:\n",
        "            lemmatized_string += lemmatizer.lemmatize(word) + \" \"\n",
        "    return lemmatized_string"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trP1bonq4E1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####DATA PREPROCESSING####\n",
        "#Appending reviews and its corresponding label to different lists for each set\n",
        "dev_processed_text = []\n",
        "dev_processed_label = []\n",
        "train_processed_text = []\n",
        "train_processed_label = []\n",
        "test_processed_text = []\n",
        "test_processed_label = []\n",
        "for i in train_set:\n",
        "    train_processed_text.append((clean_reviews(i[0])))\n",
        "    train_processed_label.append(i[1])  \n",
        "for i in test_set:\n",
        "    test_processed_text.append((clean_reviews(i[0])))\n",
        "    test_processed_label.append(i[1])\n",
        "for i in dev_set:\n",
        "    dev_processed_text.append((clean_reviews(i[0])))\n",
        "    dev_processed_label.append(i[1])\n",
        "\n",
        "y_train = train_processed_label    \n",
        "y_test = test_processed_label\n",
        "y_dev = dev_processed_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgfG0A436Xd5",
        "colab_type": "text"
      },
      "source": [
        "Feature Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPt6b-uu4iD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(ngram_range = (1,2)) #Intialize TF-IDF vectorizer and ngram features\n",
        "X_train = tfidf_vectorizer.fit_transform(train_processed_text) #Fit and transform our training set reviews using the created TF-IDF vectorize\n",
        "X_test = tfidf_vectorizer.transform(test_processed_text) #Fit and transform our test set reviews using the created TF-IDF vectorize\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORd_mZJM7Kze",
        "colab_type": "text"
      },
      "source": [
        "Model Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW27Qx017P2i",
        "colab_type": "code",
        "outputId": "de6c07a1-6fb3-46ea-e17f-3ab30d81be2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "#Logistic Regression\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "#Predict on test set using our model\n",
        "y_pred = base_model.predict(X_test)\n",
        "\n",
        "#Calculate precision, recall, f1 score and accuracy of the model\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "#Print the evaluation measures\n",
        "print (\"Precision: \"+str(round(precision,5)))\n",
        "print (\"Recall: \"+str(round(recall,5)))\n",
        "print (\"F1-Score: \"+str(round(f1,5)))\n",
        "print (\"Accuracy: \"+str(round(accuracy,5)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.31532\n",
            "Recall: 0.20904\n",
            "F1-Score: 0.21356\n",
            "Accuracy: 0.32556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L8KvlctEg1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create a SVM Classifier\n",
        "#Initalize SVM CLassifier\n",
        "svm_clf= sklearn.svm.SVC()\n",
        "\n",
        "#Train the classifier\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "#Predict on test set\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "\n",
        "#Calculate precision, recall, f1 score and accuracy of the model\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_test, y_pred) \n",
        "\n",
        "#Print the evaluation measures\n",
        "print (\"Precision: \"+str(round(precision,5)))\n",
        "print (\"Recall: \"+str(round(recall,5)))\n",
        "print (\"F1-Score: \"+str(round(f1,5)))\n",
        "print (\"Accuracy: \"+str(round(accuracy,5)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_BzMBI7p0Nw",
        "colab_type": "code",
        "outputId": "670b6130-df86-4dd6-b4e7-d7812c563400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "train_sentence_embedding = []\n",
        "for i in train_text[0:1]:\n",
        "  x = bert.embed(i)\n",
        "  train_sentence_embedding.append(x)\n",
        "print(train_sentence_embedding)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ae265e12a97b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_sentence_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtrain_sentence_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sentence_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFglKHn9m5zA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}